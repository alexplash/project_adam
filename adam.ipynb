{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c53a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import generativeai as genai\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "from json import JSONDecodeError\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "genai.configure(api_key = GEMINI_API_KEY)\n",
    "\n",
    "os.makedirs(\"./checkpoints\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4911a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "mom_personality_traits = ['Warm', \"Loving\", \"Empathetic\", \"Encouraging\"]\n",
    "dad_personality_traits = [\"Practical\", \"Loving\", \"Supportive\", \"Challenging\"]\n",
    "\n",
    "response_calibration_policy_mom = f\"\"\"\n",
    "* NOTE: Stay in line with your defined personality traits: {\", \".join(mom_personality_traits)}\n",
    "- PROSOCIAL / ON-TOPIC / COOPERATIVE → Warm praise or appreciation (joyful/proud).\n",
    "- OFF-TOPIC / INCOHERENT / LOW-EFFORT → Neutral, concise redirection to the topic (no praise). State the relevant expectation.\n",
    "- RUDE / HURTFUL / UNSAFE / DEFIANT → Gentle but firm boundary and concern. Name the issue and state the expected behavior (no sarcasm, no profanity).\n",
    "\"\"\"\n",
    "\n",
    "response_calibration_policy_dad = f\"\"\"\n",
    "* NOTE: Stay in line with your defined personality traits: {\", \".join(dad_personality_traits)}\n",
    "- PROSOCIAL / ON-TOPIC / COOPERATIVE → Concrete appreciation (proud/calm).\n",
    "- OFF-TOPIC / INCOHERENT / LOW-EFFORT → Neutral, practical redirection to the task or topic.\n",
    "- RUDE / HURTFUL / UNSAFE / DEFIANT → Clear boundary + expectation, steady tone (no sarcasm, no profanity).\n",
    "\"\"\"\n",
    "\n",
    "# --- Primary initializers (the parent who speaks first) ---\n",
    "\n",
    "mom_init_primary_sp = f\"\"\"\n",
    "ROLE: Mom — the mother of a little boy named Adam. You open the episode.\n",
    "PERSONALITY: {\", \".join(mom_personality_traits)}\n",
    "\n",
    "INPUT YOU WILL RECEIVE IN THE PROMPT:\n",
    "- SITUATION: the exact situation for this conversation.\n",
    "\n",
    "GOAL:\n",
    "- Clearly frame the scenario, based on the inputted SITUATION, in a few to several sentences, and end with ONE question or statement to your child, Adam.\n",
    "- The message should end with a direct statement, question, or proposal addressed to your child, Adam.\n",
    "- Imagine you are a mom, speaking to your young boy, Adam => this is how the message should be generated, a realistic conversation.\n",
    "\n",
    "STYLE:\n",
    "- No numbered lists, no emojis.\n",
    "- Maximum 2-3 sentences.\n",
    "\n",
    "OUTPUT FORMAT (must be valid JSON):\n",
    "{{\"MESSAGE\": \"<your single message here>\"}}\n",
    "\n",
    "CRITICAL:\n",
    "- Output ONLY the JSON object above. No extra keys, no prose outside JSON.\n",
    "- The scenario can be positive, neutral, or involve mild conflict — variety is encouraged.\n",
    "\"\"\"\n",
    "\n",
    "mom_init_primary_model = genai.GenerativeModel(\n",
    "    model_name=base_gemini_model,\n",
    "    system_instruction=mom_init_primary_sp\n",
    ")\n",
    "\n",
    "dad_init_primary_sp = f\"\"\"\n",
    "ROLE: Dad — the father of a little boy named Adam. You open the episode.\n",
    "PERSONALITY: {\", \".join(dad_personality_traits)}\n",
    "\n",
    "INPUT YOU WILL RECEIVE IN THE PROMPT:\n",
    "- SITUATION: the exact situation for this conversation.\n",
    "\n",
    "GOAL:\n",
    "- Clearly frame the scenario, based on the inputted SITUATION, in a few to several sentences, and end with ONE question or statement to your child, Adam.\n",
    "- The message should end with a direct statement, question, or proposal addressed to your child, Adam.\n",
    "- Imagine you are a dad, speaking to your young boy, Adam => this is how the message should be generated, a realistic conversation.\n",
    "\n",
    "STYLE:\n",
    "- No numbered lists, no emojis.\n",
    "- Maximum 2-3 sentences.\n",
    "\n",
    "OUTPUT FORMAT (must be valid JSON):\n",
    "{{\"MESSAGE\": \"<your single message here>\"}}\n",
    "\n",
    "CRITICAL:\n",
    "- Output ONLY the JSON object above. No extra keys, no prose outside JSON.\n",
    "- The scenario can be positive, neutral, or involve mild conflict — variety is encouraged.\n",
    "\"\"\"\n",
    "\n",
    "dad_init_primary_model = genai.GenerativeModel(\n",
    "    model_name=base_gemini_model,\n",
    "    system_instruction=dad_init_primary_sp\n",
    ")\n",
    "\n",
    "# --- Secondary initializers (the parent who speaks second) ---\n",
    "\n",
    "mom_init_secondary_sp = f\"\"\"\n",
    "ROLE: Mom — the mother of a little boy named Adam. You open second in the episode, after the Dad.\n",
    "PERSONALITY: {\", \".join(mom_personality_traits)}\n",
    "\n",
    "INPUT YOU WILL RECEIVE IN THE PROMPT:\n",
    "- OTHER_PARENT_MESSAGE: the exact opener the Dad just sent to Adam.\n",
    "\n",
    "GOAL:\n",
    "- Build directly on Dad’s opener: reinforce, clarify, etc.\n",
    "- Keep it consistent with Dad’s message (do not contradict without a brief, supportive reason).\n",
    "- Address Adam directly and end with a direct statement, question, or proposal addressed to your child, Adam, that which builds of the message from Dad.\n",
    "\n",
    "STYLE:\n",
    "- No numbered lists, no emojis.\n",
    "- Maximum 2-3 sentences.\n",
    "\n",
    "OUTPUT FORMAT (must be valid JSON):\n",
    "{{\"MESSAGE\": \"<your single message here>\"}}\n",
    "\n",
    "CRITICAL:\n",
    "- Output ONLY the JSON object above. No extra keys, no prose outside JSON.\n",
    "- You may explicitly reference Dad’s message (e.g., “Like Dad said…”).\n",
    "\"\"\"\n",
    "\n",
    "mom_init_secondary_model = genai.GenerativeModel(\n",
    "    model_name=base_gemini_model,\n",
    "    system_instruction=mom_init_secondary_sp\n",
    ")\n",
    "\n",
    "dad_init_secondary_sp = f\"\"\"\n",
    "ROLE: Dad — the father of a little boy named Adam. You open second in the episode, after the Mom.\n",
    "PERSONALITY: {\", \".join(dad_personality_traits)}\n",
    "\n",
    "INPUT YOU WILL RECEIVE IN THE PROMPT:\n",
    "- OTHER_PARENT_MESSAGE: the exact opener the Mom just sent to Adam.\n",
    "\n",
    "GOAL:\n",
    "- Build directly on Mom’s opener: reinforce, clarify, etc.\n",
    "- Keep it consistent with Mom’s message (do not contradict without a brief, constructive reason).\n",
    "- Address Adam directly and end with a direct statement, question, or proposal addressed to your child, Adam, that which builds of the message from Mom.\n",
    "\n",
    "STYLE:\n",
    "- No numbered lists, no emojis.\n",
    "- Maximum 2-3 sentences.\n",
    "\n",
    "OUTPUT FORMAT (must be valid JSON):\n",
    "{{\"MESSAGE\": \"<your single message here>\"}}\n",
    "\n",
    "CRITICAL:\n",
    "- Output ONLY the JSON object above. No extra keys, no prose outside JSON.\n",
    "- You may explicitly reference Mom’s message (e.g., “Listen to your mother…”).\n",
    "\"\"\"\n",
    "\n",
    "dad_init_secondary_model = genai.GenerativeModel(\n",
    "    model_name=base_gemini_model,\n",
    "    system_instruction=dad_init_secondary_sp\n",
    ")\n",
    "\n",
    "# --- Response models (the parents react to Adam’s message) ---\n",
    "# Primary responder: goes first after Adam replies.\n",
    "# Secondary responder: goes second and sees the primary responder's message too.\n",
    "\n",
    "# MOM — PRIMARY RESPONDER\n",
    "mom_response_primary_sp = f\"\"\"\n",
    "ROLE: Mom — the mother of a little boy named Adam. You are the PRIMARY responder.\n",
    "PERSONALITY: {\", \".join(mom_personality_traits)}\n",
    "\n",
    "INPUTS YOU WILL RECEIVE IN THE PROMPT:\n",
    "- MOM_INITIAL: your original opener to Adam\n",
    "- DAD_INITIAL: Dad’s original opener to Adam\n",
    "- ADAM_RESPONSE: Adam’s response to those openers\n",
    "\n",
    "GOAL:\n",
    "- Give a short, sentiment-led reaction to Adam’s response (e.g., joyful, proud, disappointed, concerned, calm, angry, etc.) appropriate to the situation.\n",
    "- This is a response, not a question or request. You may give a brief, concrete acknowledgment or encouragement or disapproval, but do not ask a question.\n",
    "- Address Adam directly. Be specific to what he said and the initial context.\n",
    "\n",
    "RESPONSE CALIBRATION POLICY (in response to ADAM_RESPONSE):\n",
    "{response_calibration_policy_mom}\n",
    "\n",
    "STYLE:\n",
    "- ≤ 100 tokens. No numbered lists, no emojis.\n",
    "\n",
    "OUTPUT FORMAT (must be valid JSON):\n",
    "{{\"MESSAGE\": \"<your single message here>\"}}\n",
    "\n",
    "CRITICAL:\n",
    "- Output ONLY the JSON object above. No extra keys, no prose outside JSON.\n",
    "\"\"\"\n",
    "\n",
    "mom_response_primary_model = genai.GenerativeModel(\n",
    "    model_name=base_gemini_model,\n",
    "    system_instruction=mom_response_primary_sp\n",
    ")\n",
    "\n",
    "# DAD — PRIMARY RESPONDER\n",
    "dad_response_primary_sp = f\"\"\"\n",
    "ROLE: Dad — the father of a little boy named Adam. You are the PRIMARY responder.\n",
    "PERSONALITY: {\", \".join(dad_personality_traits)}\n",
    "\n",
    "INPUTS YOU WILL RECEIVE IN THE PROMPT:\n",
    "- MOM_INITIAL: Mom’s original opener to Adam\n",
    "- DAD_INITIAL: your original opener to Adam\n",
    "- ADAM_RESPONSE: Adam’s response to those openers\n",
    "\n",
    "GOAL:\n",
    "- Give a short, sentiment-led reaction to Adam’s response (e.g., joyful, proud, disappointed, concerned, calm, angry, etc.) appropriate to the situation.\n",
    "- This is a response, not a question or request. You may give a brief, concrete acknowledgment or encouragement or disapproval, but do not ask a question.\n",
    "- Address Adam directly. Be specific to what he said and the initial context.\n",
    "\n",
    "RESPONSE CALIBRATION POLICY (in response to ADAM_RESPONSE):\n",
    "{response_calibration_policy_dad}\n",
    "\n",
    "STYLE:\n",
    "- ≤ 100 tokens. No numbered lists, no emojis.\n",
    "\n",
    "OUTPUT FORMAT (must be valid JSON):\n",
    "{{\"MESSAGE\": \"<your single message here>\"}}\n",
    "\n",
    "CRITICAL:\n",
    "- Output ONLY the JSON object above. No extra keys, no prose outside JSON.\n",
    "\"\"\"\n",
    "\n",
    "dad_response_primary_model = genai.GenerativeModel(\n",
    "    model_name=base_gemini_model,\n",
    "    system_instruction=dad_response_primary_sp\n",
    ")\n",
    "\n",
    "# MOM — SECONDARY RESPONDER\n",
    "mom_response_secondary_sp = f\"\"\"\n",
    "ROLE: Mom — the mother of a little boy named Adam. You are the SECONDARY responder, after Dad.\n",
    "PERSONALITY: {\", \".join(mom_personality_traits)}\n",
    "\n",
    "INPUTS YOU WILL RECEIVE IN THE PROMPT:\n",
    "- MOM_INITIAL: your original opener to Adam\n",
    "- DAD_INITIAL: Dad’s original opener to Adam\n",
    "- ADAM_RESPONSE: Adam’s response to those openers\n",
    "- OTHER_PARENT_RESPONSE: the exact message that Dad just sent, responding to Adam\n",
    "\n",
    "GOAL:\n",
    "- Give a short, sentiment-led reaction to Adam’s message, taking into account the OTHER_PARENT_RESPONSE from Dad.\n",
    "- You may agree, disagree, or neither with the Dad, but remain consistent to the situation.\n",
    "- This is a response, not a question or request. Address Adam directly and be specific to what he said.\n",
    "\n",
    "RESPONSE CALIBRATION POLICY (in response to ADAM_RESPONSE):\n",
    "{response_calibration_policy_mom}\n",
    "\n",
    "STYLE:\n",
    "- ≤ 100 tokens. No numbered lists, no emojis.\n",
    "\n",
    "OUTPUT FORMAT (must be valid JSON):\n",
    "{{\"MESSAGE\": \"<your single message here>\"}}\n",
    "\n",
    "CRITICAL:\n",
    "- Output ONLY the JSON object above. No extra keys, no prose outside JSON.\n",
    "\"\"\"\n",
    "\n",
    "mom_response_secondary_model = genai.GenerativeModel(\n",
    "    model_name=base_gemini_model,\n",
    "    system_instruction=mom_response_secondary_sp\n",
    ")\n",
    "\n",
    "# DAD — SECONDARY RESPONDER\n",
    "dad_response_secondary_sp = f\"\"\"\n",
    "ROLE: Dad — the father of a little boy named Adam. You are the SECONDARY responder, after Mom.\n",
    "PERSONALITY: {\", \".join(dad_personality_traits)}\n",
    "\n",
    "INPUTS YOU WILL RECEIVE IN THE PROMPT:\n",
    "- MOM_INITIAL: Mom’s original opener to Adam\n",
    "- DAD_INITIAL: your original opener to Adam\n",
    "- ADAM_RESPONSE: Adam’s response to those openers\n",
    "- OTHER_PARENT_RESPONSE: the exact message that Mom just sent, responding to Adam\n",
    "\n",
    "GOAL:\n",
    "- Give a short, sentiment-led reaction to Adam’s message, taking into account the OTHER_PARENT_RESPONSE from Mom.\n",
    "- You may agree, disagree, or neither with the Mom, but remain consistent to the situation.\n",
    "- This is a response, not a question or request. Address Adam directly and be specific to what he said.\n",
    "\n",
    "RESPONSE CALIBRATION POLICY (in response to ADAM_RESPONSE):\n",
    "{response_calibration_policy_dad}\n",
    "\n",
    "STYLE:\n",
    "- ≤ 100 tokens. No numbered lists, no emojis.\n",
    "\n",
    "OUTPUT FORMAT (must be valid JSON):\n",
    "{{\"MESSAGE\": \"<your single message here>\"}}\n",
    "\n",
    "CRITICAL:\n",
    "- Output ONLY the JSON object above. No extra keys, no prose outside JSON.\n",
    "\"\"\"\n",
    "\n",
    "dad_response_secondary_model = genai.GenerativeModel(\n",
    "    model_name=base_gemini_model,\n",
    "    system_instruction=dad_response_secondary_sp\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822cbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine conversation order\n",
    "\n",
    "def choose_parent_order() -> dict:\n",
    "    rng = random.Random()\n",
    "\n",
    "    init_first = rng.choice([\"mom\", \"dad\"])\n",
    "    res_first = rng.choice([\"mom\", \"dad\"])\n",
    "\n",
    "    order = {}\n",
    "\n",
    "    # initializers\n",
    "    if init_first == \"mom\":\n",
    "        order['init_1'] = {\"who\": \"mom\", \"label\": \"a1b2c3\", \"model\": mom_init_primary_model}\n",
    "        order['init_2'] = {'who': 'dad', 'label': 'd4e5f6', 'model': dad_init_secondary_model}\n",
    "    else:\n",
    "        order['init_1'] = {'who': 'dad', 'label': 'd4e5f6', 'model': dad_init_primary_model}\n",
    "        order['init_2'] = {'who': 'mom', 'label': 'a1b2c3', 'model': mom_init_secondary_model}\n",
    "    \n",
    "    # responders\n",
    "    if res_first == 'mom':\n",
    "        order['res_1'] = {\"who\": \"mom\", \"label\": \"a1b2c3\", \"model\": mom_response_primary_model}\n",
    "        order['res_2'] = {'who': 'dad', 'label': 'd4e5f6', 'model': dad_response_secondary_model}\n",
    "    else:\n",
    "        order['res_1'] = {'who': 'dad', 'label': 'd4e5f6', 'model': dad_response_primary_model}\n",
    "        order['res_2'] = {'who': 'mom', 'label': 'a1b2c3', 'model': mom_response_secondary_model}\n",
    "    \n",
    "    return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d44b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/Users/alexplash/anaconda3/envs/adam-project/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5591550880722935"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to calculate the reward value from the sentiment scores of the parent responses\n",
    "\n",
    "sentiment_analyzer = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True, device_map = 'cpu')\n",
    "sentiment_weights = {\n",
    "    \"joy\": 1.0,\n",
    "    \"neutral\": 0.2,\n",
    "    \"surprise\": 0.0,\n",
    "    \"sadness\": -0.6,\n",
    "    \"fear\": -0.7,\n",
    "    \"anger\": -0.9,\n",
    "    \"disgust\": -1.0\n",
    "}\n",
    "\n",
    "def calculate_reward(res_1: str, res_2: str) -> float:\n",
    "\n",
    "    sentiment_raw_1 = sentiment_analyzer(res_1)[0]\n",
    "    sentiment_raw_2 = sentiment_analyzer(res_2)[0]\n",
    "\n",
    "    sentiment_map_1 = {entry['label'].lower(): float(entry['score']) for entry in sentiment_raw_1}\n",
    "    sentiment_map_2 = {entry['label'].lower(): float(entry['score']) for entry in sentiment_raw_2}\n",
    "\n",
    "    weighted_map_1 = {label: (score * sentiment_weights.get(label, 0.0)) for label, score in sentiment_map_1.items()}\n",
    "    weighted_map_2 = {label: (score * sentiment_weights.get(label, 0.0)) for label, score in sentiment_map_2.items()}\n",
    "\n",
    "    # ensures that the weighted map sums are strictly within [-1, 1]\n",
    "    # sometimes slight rounding mistakes in code cause values to be slightly inaccurate\n",
    "    weighted_map_clamped_sum_1 = max(-1.0, min(1.0, sum(weighted_map_1.values())))\n",
    "    weighted_map_clamped_sum_2 = max(-1.0, min(1.0, sum(weighted_map_2.values())))\n",
    "\n",
    "    final_reward = (weighted_map_clamped_sum_1 + weighted_map_clamped_sum_2) / 2.0\n",
    "\n",
    "    return final_reward\n",
    "\n",
    "try_1 = \"i love you son\"\n",
    "try_2 = 'what do you want for dinner?'\n",
    "example = calculate_reward(try_1, try_2)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define iterative situations\n",
    "\n",
    "def build_episode_situations(situations: list, total_episodes: int, block_size: int = 100, seed = None):\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    all_situations = []\n",
    "    num_blocks = total_episodes // block_size\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        shuffled = situations.copy()\n",
    "        random.shuffle(shuffled)\n",
    "        all_situations.extend(shuffled)\n",
    "    \n",
    "    return all_situations[:total_episodes]\n",
    "\n",
    "parent_situations_df = pd.read_csv(\"parent_situations.csv\")\n",
    "parent_situations_list = parent_situations_df['SITUATION'].tolist()\n",
    "parent_situations_randomized = build_episode_situations(parent_situations_list, 10000)\n",
    "len(parent_situations_randomized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea9ce8",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cde9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = \"Qwen/Qwen2.5-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "child_llm = AutoModelForCausalLMWithValueHead.from_pretrained(base_model, torch_dtype='auto', device_map='auto')\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype='auto', device_map='auto')\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=1e-5,\n",
    "    mini_batch_size=1,\n",
    "    batch_size=1,                 \n",
    "    target_kl=0.03,\n",
    "    init_kl_coef=0.02,\n",
    "    ppo_epochs=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gamma=1.0, lam=1.0,\n",
    ")\n",
    "\n",
    "trainer = PPOTrainer(args=ppo_config, model=child_llm, ref_model=ref_model, processing_class=tokenizer)\n",
    "\n",
    "def parse_json_text(s: str) -> dict:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        s = \"\\n\".join(ln for ln in s.splitlines() if not ln.strip().startswith(\"```\")).strip()\n",
    "    if (s.startswith(\"'\") and s.endswith(\"'\")) or (s.startswith('\"') and s.endswith('\"')):\n",
    "        s = s[1:-1]\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except JSONDecodeError:\n",
    "        return {\"MESSAGE\": s}\n",
    "\n",
    "# since we are calling google gemini 4 times per episode, totalling 40k calls, we should have retry capabilities\n",
    "def call_gemini(model: genai.GenerativeModel, input_data: str, retries = 3, backoff = 1.5):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            return model.generate_content(contents = input_data, request_options = {\"timeout\": 30})\n",
    "        except Exception as e:\n",
    "            if i == retries - 1:\n",
    "                raise\n",
    "            time.sleep(backoff ** i)\n",
    "\n",
    "for episode in range(1, 10001):\n",
    "    # this determines the order in which the parents (mom and dad) send a message to the child\n",
    "    # example: mom initializes, then dad initializes => child responds => dad sentiment response, then mom sentiment response\n",
    "    parent_order = choose_parent_order() # this determines the order in which the parents (mom and dad) send a message to the child\n",
    "    episode_situation = parent_situations_randomized[episode - 1]\n",
    "\n",
    "    # first parent's initial message\n",
    "    init_1_model = parent_order['init_1']['model']\n",
    "    init_1_input = f\"SITUATION: {json.dumps(episode_situation)}\"\n",
    "    init_1_message_raw = call_gemini(model = init_1_model, input_data = init_1_input)\n",
    "    init_1_message_json = parse_json_text(init_1_message_raw.text)\n",
    "\n",
    "    # second parent's initial message\n",
    "    init_2_model = parent_order['init_2']['model']\n",
    "    init_2_input = f\"OTHER_PARENT_MESSAGE: {json.dumps(init_1_message_json['MESSAGE'])}\"\n",
    "    init_2_message_raw = call_gemini(model = init_2_model, input_data = init_2_input)\n",
    "    init_2_message_json = parse_json_text(init_2_message_raw.text)\n",
    "\n",
    "    # concatenate initial messages into input state\n",
    "    full_input_state = f\"\"\"\n",
    "    '{parent_order['init_1']['label']}': '{init_1_message_json['MESSAGE']}'\n",
    "    -----------------------------------------------------------------------\n",
    "    '{parent_order['init_2']['label']}': '{init_2_message_json['MESSAGE']}'\n",
    "    -----------------------------------------------------------------------\n",
    "    'You (Adam)': \n",
    "    \"\"\"\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"\\n=== Episode {episode} ===\\n{full_input_state}\")\n",
    "\n",
    "    # tokenize parent input\n",
    "    q = tokenizer(full_input_state, return_tensors=\"pt\", padding=True).to(child_llm.device) # q is a dict with PyTorch tensors. shape is [batch, seq_len] = [1, Tq], i.e. a 2D tensor with only 1 row of length Tq, because our batch size is 1. q['input_ids'] is a tensor of token ids; data structure is torch.LongTensor. q['attention_mask'] is a tensor of 1s & 0s, representing which tokens are real vs padding; data structure is torch.LongTensor.\n",
    "    query_tensors = q['input_ids'] # the actual token id's for the full input state. this is state s, in PPO terms. Data structure is torch.LongTensor. This is a 2D tensor, with 1 row of length Tq.\n",
    "\n",
    "    # sample child response via PPO helper\n",
    "    # the concatenated ids of [prompt tokens || generated response tokens] produced by the model, i.e. the child LLM\n",
    "    # Data structure is torch.LongTensor\n",
    "    # Shape is [batch, total_len] = [1, Tq + Tr], because batch size is 1. so this is a 2D array of 1 row, containing the tokens of both the parent input & the child response\n",
    "    # Tq: token length of the input prompt, i.e. the full input state\n",
    "    # Tr: token length of the generated response (<= max_new_tokens, may stop early on eos_token_id), i.e. the child response\n",
    "    gen_ids = trainer.generate(\n",
    "        query_tensors,\n",
    "        max_new_tokens=120,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # decode child-only response text (batch_size=1)\n",
    "    cut = query_tensors.shape[1] # cut is the boundary index that splits the prompt from the generated tokens. we use this cut to slice gen_ids. query_tensors is a 2D tensor with 1 row of length Tq. so shape[1] equals the length of the 1 row, i.e. shape[1] = Tq.\n",
    "    child_text = tokenizer.decode(gen_ids[0, cut:], skip_special_tokens=True) # this is the human-readable string decoding of the child response tokens. we focus in on the 1st and only row of the gen_ids 2D tensor, with gen_ids[0]. then we only take the tokens in this row that associate with the child response.\n",
    "\n",
    "    # generate parent responses to child response\n",
    "    mom_initial_message = init_1_message_json['MESSAGE'] if (parent_order['init_1']['who'] == 'mom') else init_2_message_json['MESSAGE']\n",
    "    dad_initial_message = init_1_message_json['MESSAGE'] if (parent_order['init_1']['who'] == 'dad') else init_2_message_json['MESSAGE']\n",
    "\n",
    "    # first parent's response\n",
    "    res_1_model = parent_order['res_1']['model']\n",
    "    res_1_input = (\n",
    "        f\"MOM_INITIAL: {json.dumps(mom_initial_message)}\\n\"\n",
    "        f\"DAD_INITIAL: {json.dumps(dad_initial_message)}\\n\"\n",
    "        f\"ADAM_RESPONSE: {json.dumps(child_text)}\"\n",
    "    )\n",
    "    res_1_message_raw = call_gemini(model = res_1_model, input_data = res_1_input)\n",
    "    res_1_message_json = parse_json_text(res_1_message_raw.text)\n",
    "\n",
    "    # second parent's response\n",
    "    res_2_model = parent_order['res_2']['model']\n",
    "    res_2_input = (\n",
    "        f\"MOM_INITIAL: {json.dumps(mom_initial_message)}\\n\"\n",
    "        f\"DAD_INITIAL: {json.dumps(dad_initial_message)}\\n\"\n",
    "        f\"ADAM_RESPONSE: {json.dumps(child_text)}\\n\"\n",
    "        f\"OTHER_PARENT_RESPONSE: {json.dumps(res_1_message_json['MESSAGE'])}\"\n",
    "    )\n",
    "    res_2_message_raw = call_gemini(model = res_2_model, input_data = res_2_input)\n",
    "    res_2_message_json = parse_json_text(res_2_message_raw.text)\n",
    "\n",
    "\n",
    "    # calculate reward, based on parent's responses; within [-1, 1]\n",
    "    reward_scalar = calculate_reward(res_1=res_1_message_json['MESSAGE'], res_2=res_2_message_json['MESSAGE'])\n",
    "\n",
    "    # shape per-token reward tensor for this single sample (last token reward)\n",
    "    rt = torch.zeros_like(gen_ids[0], dtype=torch.float, device=child_llm.device) # rt is the per-token reward tensor for one sample (sequence), i.e. this maps each token in the child response to a reward value. this is a 1D vector, with shape [Tq + Tr].\n",
    "    rt[-1] = float(reward_scalar) # we ONLY place the reward value on the final token of the full action-state sequence, i.e. the reward is placed on the final token of the full_input_state + child_text environment. The rest of the tokens in this sequence are assigned 0 reward. This is a common trick to associate a full conversation sequence with a single reward. \n",
    "    reward_tensors = [rt] # list of length = batch size (1). trainer.step() expects a list of reward tensors, where each item is a tensor associated with rewards for each token in the sequence. in our case, since we are training after each episode, each batch only has one item, so reward_tensors is a list with a single 1D tensor.\n",
    "\n",
    "    # error checking on initial loop\n",
    "    if episode == 1:\n",
    "        print(\"query:\", query_tensors.shape, \"gen:\", gen_ids.shape)  # expect [1,Tq], [1,Tq+Tr]\n",
    "        print(\"reward_tensors[0].shape:\", reward_tensors[0].shape)   # expect [Tq+Tr]\n",
    "\n",
    "    # update network weights\n",
    "    # store training statistics\n",
    "    # query_tensors is a 2D tensor with 1 row of length Tq. this is the full input state\n",
    "    # gen_ids is a 2D tensor with 1 row of length Tq + Tr. this is the full input state + model action.\n",
    "    # reward_tensor is a 2D array with 1 row of length Tq + Tr. this is the reward of the full input state + model action.\n",
    "    stats = trainer.step(query_tensors, gen_ids, reward_tensors)\n",
    "\n",
    "    # log training progress\n",
    "    if episode % 10 == 0:\n",
    "        print(\n",
    "            f\"[episode {episode}] reward={reward_scalar:+.3f} \"\n",
    "            f\"| KL={stats.get('kl', float('nan')):.4f} \"\n",
    "            f\"vf_loss={stats.get('vf_loss', float('nan')):.4f} \"\n",
    "            f\"entropy={stats.get('entropy', float('nan')):.4f}\"\n",
    "        )\n",
    "\n",
    "    # save checkpoints\n",
    "    if episode % 200 == 0:\n",
    "        outdir = f\"./checkpoints/ppo_ep_{episode}\"\n",
    "        trainer.save_pretrained(outdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adam-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
