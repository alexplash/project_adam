{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSVFfKhiZgrS",
        "outputId": "e7ea5ccf-555a-4c25-8a4b-ad64a66da181"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install --no-cache-dir torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --extra-index-url https://download.pytorch.org/whl/cu124\n",
        "!{sys.executable} -m pip install --no-cache-dir transformers==4.42.3 tqdm numpy\n",
        "!{sys.executable} -m pip install --no-cache-dir bitsandbytes==0.43.3 datasets==3.0.1 wandb\n",
        "!{sys.executable} -m pip install --no-cache-dir openai\n",
        "!{sys.executable} -m pip install python-dotenv\n",
        "!{sys.executable} -m pip install accelerate\n",
        "!{sys.executable} -m pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMQLE5vDaRHG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoConfig,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer\n",
        ")\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import requests\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Disable ONLY console log capture\n",
        "os.environ[\"WANDB_CONSOLE\"] = \"off\"\n",
        "\n",
        "# Disable code saving (keeps runs clean)\n",
        "os.environ[\"WANDB_DISABLE_CODE\"] = \"true\"\n",
        "\n",
        "# Disable system metrics (saves overhead)\n",
        "os.environ[\"WANDB_DISABLE_SERVICE\"] = \"true\"\n",
        "\n",
        "# OPTIONAL: Stop wandb from watching model gradients\n",
        "os.environ[\"WANDB_WATCH\"] = \"false\"\n",
        "\n",
        "wandb.init(project=\"project-adam\", name=\"copy-gpt2-pgsrm\")\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "bot_id = os.getenv(\"BOT_ID\")\n",
        "chat_id = os.getenv(\"CHAT_ID\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZpl9JQdpsBK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class PPOTrainer:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      actor_model: PreTrainedModel,\n",
        "      ref_model: PreTrainedModel,\n",
        "      tokenizer: PreTrainedTokenizer,\n",
        "      batch_size: int,\n",
        "      actor_learning_rate: float = 1e-5,\n",
        "      critic_learning_rate: float = 1e-5,\n",
        "      clip_range: float = 0.2,\n",
        "      value_coef: float = 0.5,\n",
        "      entropy_coef: float = 0.01,\n",
        "      kl_coef: float = 0.05,\n",
        "      target_kl: float = 0.05,\n",
        "      max_grad_norm: float = 1.0,\n",
        "      device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    ):\n",
        "\n",
        "    self.actor = actor_model.to(device)\n",
        "    self.ref = ref_model.to(device)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.device = device\n",
        "\n",
        "    # freeze the reference model\n",
        "    self.ref.eval()\n",
        "    for param in self.ref.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "\n",
        "    # here, we are creating a single perceptron for the critic head\n",
        "    # this perceptron maps the vector representing the \"meaning\" of the action (vector of size hidden_state; gpt-medium is 1024), to a single output node\n",
        "    # this output node is the predicted reward for the action\n",
        "    hidden_size = self.actor.config.hidden_size\n",
        "    self.critic = nn.Linear(hidden_size, 1).to(device).float()\n",
        "\n",
        "    self.clip_range = clip_range\n",
        "    self.value_coef = value_coef\n",
        "    self.entropy_coef = entropy_coef\n",
        "    self.kl_coef = kl_coef\n",
        "    self.target_kl = target_kl\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    # max grad norm is basically the cumulative max for gradients at each training step\n",
        "    # this is sort of like clipping, but for the cumulative gradients calculated at each step\n",
        "    self.max_grad_norm = max_grad_norm\n",
        "\n",
        "    # we set up 2 different optimizers for the actor and critic, because both have different loss functions, and function separately\n",
        "    # for the actor, the loss function is negative log-loss\n",
        "    # for the critic, the loss function is MSE\n",
        "    self.actor_optimizer = AdamW(self.actor.parameters(), lr = actor_learning_rate)\n",
        "    self.critic_optimizer = AdamW(self.critic.parameters(), lr = critic_learning_rate)\n",
        "\n",
        "  # ---- Helper Functions ---- #\n",
        "  @staticmethod\n",
        "  # this is the softmax function, converting vector of logits into a vector log probability distribution\n",
        "  def logprobs_from_logits(logits, labels):\n",
        "    # logprobs shape: (batch_size, seq_len - 1, vocab_size)\n",
        "    logprobs = F.log_softmax(logits, dim = -1) # this converts logit vector for all tokens into log prob distribution. dim = - 1 specifies to do this calculation along the \"last\" axis, which in this case is the vocabulary dimension, i.e. the vector of logits\n",
        "\n",
        "    # labels.unsqueeze(-1) adds a new dimension to the labels shape (batch_size, seq_len - 1) => (batch_size, seq_len - 1, 1)\n",
        "    # gather() picks values from the logprobs object, along the -1 dimension (which is the vocab vector), based on the labels object (batch_size, seq_len - 1, 1)\n",
        "    # basically, the output is of shape (batch_size, seq_len - 1), where we have only the picked token's log prob value, for all tokens in each batch\n",
        "    return logprobs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "  @staticmethod\n",
        "  # this is used to zero out non-response tokens within our loss function\n",
        "  # at the start of the step function, we define a mask of shape (batch_size, seq_len), with 0's for the prompt and 1's for the response\n",
        "  # this function zero's out all the prompt tokens in the loss function, so that we only optimize the actor based on it's response\n",
        "  def masked_mean(values, mask):\n",
        "    return (values * mask).sum() / mask.sum()\n",
        "\n",
        "\n",
        "\n",
        "  # ---- Training Function ----\n",
        "  def step(self, prompts, responses, rewards, average_reward):\n",
        "    \"\"\"\n",
        "    Perform one PPO optimization step.\n",
        "    Inputs:\n",
        "      prompts:   [batch_size, prompt_len]\n",
        "      responses: [batch_size, response_len],\n",
        "      rewards:   [batch_size, 1]\n",
        "    \"\"\"\n",
        "    self.actor.train()\n",
        "\n",
        "    if self.batch_size != prompts.size(0):\n",
        "      print(\"batch size must match number of prompts\")\n",
        "      return\n",
        "\n",
        "    # we concatenate the prompt tokens and the response tokens, so now the structure is [batch_size, prompt_len + response_len]\n",
        "    # this full sequence is what is fed into the transformer again\n",
        "    input_ids = torch.cat([prompts, responses], dim = 1).to(self.device)\n",
        "\n",
        "    # here we create a mask, mapping non-padded response tokens to 1, and everything else to 0\n",
        "    # this is such that when we set up our loss function, we only focus on the response of the actor\n",
        "    # example: input_ids[0] = [11, 22, 33, 44, 55, 66, 77, 88, 0, 0], where pad_token_id = 0, prompt_len = 5\n",
        "    pad_id = self.tokenizer.pad_token_id\n",
        "    prompt_len = prompts.size(1)\n",
        "    input_mask = (input_ids != pad_id).long() # input_mask[0] = [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]. just sets all non pad tokens to 1, and pad tokens to 0\n",
        "    mask = torch.zeros_like(input_ids) # mask[0] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    mask[:, prompt_len:] = input_mask[:, prompt_len:] # mask[0] = [0, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
        "    # so now, mask only weights the response tokens which are not pad tokens\n",
        "    # shape is (batch_size, seq_len)\n",
        "\n",
        "\n",
        "    # ---- Forward Pass ----\n",
        "    with torch.no_grad(): # torch.no_grad just saves compute time, because we are only doing inference for ref model, so gradient data is unnecessary\n",
        "\n",
        "      # here, we generate a tensor of size (batch_size, seq_len, vocab_size)\n",
        "      # ref_logits represents the logits for each token in the sequence, that which predict the next token, at it's position within the sequence\n",
        "      # using the reference model\n",
        "      ref_logits = self.ref(input_ids, attention_mask = input_mask).logits # we use input_mask as the attention mask, so that the model only focuses on non-padding tokens\n",
        "\n",
        "    # here, we generate a tensor of size (batch_size, seq_len, vocab_size)\n",
        "    # logits represents the logits for each token in the sequence, that which predict the next token, at it's position within the sequence\n",
        "    # we also want the list of all the hidden states from every layer within the actor model transformer\n",
        "    # using the trainable actor model\n",
        "    actor_out = self.actor(input_ids, attention_mask = input_mask, output_hidden_states = True) # we use input_mask as the attention mask, so that the model only focuses on non-padding tokens\n",
        "    logits = actor_out.logits\n",
        "\n",
        "    # hidden states is a tensor of size (batch_size, seq_len, hidden_size)\n",
        "    # it basically represents the final hidden representation (\"meaning vector\") for each token\n",
        "    # shape (batch_size, seq_len, hidden_size)\n",
        "    hidden_states = actor_out.hidden_states[-1]\n",
        "\n",
        "    # ---- Log probabilities & values ----\n",
        "    # we calculate the log probability distributions for the actor and reference models\n",
        "\n",
        "    # logits[:, :-1] is the logits for all tokens in the sequence, for all items in the batch; omit last logit, because last token has nothing to predict\n",
        "    # input_ids[:, 1:] is the actual tokens within the sequence, for all items in the batch; skip the first one, in order to compare to logits\n",
        "    # basically, we are comparing the predicted logits of the preceding token, with the actual proceeding token\n",
        "    logprobs_actor = self.logprobs_from_logits(logits[:, :-1], input_ids[:, 1:])\n",
        "    logprobs_ref = self.logprobs_from_logits(ref_logits[:, :-1], input_ids[:, 1:])\n",
        "\n",
        "    # here we generate the critic prediction for the reward of the prompt-response pairs\n",
        "    # we want the generate the reward prediction based on the hidden state of ONLY the final non-padding response token for each sequence\n",
        "    rev_mask = torch.flip(mask, dims=[1]) # first we flip our mask left-right for each sequence, so that we can get the index of the final non-padding response token\n",
        "    last_nonpad_from_end = rev_mask.float().argmax(dim=1) # returns the first occurence of 1 within the reversed mask. i.e., the index of the final non-padding response token\n",
        "    seq_len = mask.size(1)\n",
        "    last_nonpad_indices = (seq_len - 1) - last_nonpad_from_end # this is of shape (batch_size), where we have the index of the final non-padding response token for each batch\n",
        "    batch_indices = torch.arange(hidden_states.size(0), device = hidden_states.device)\n",
        "    last_hidden = hidden_states[batch_indices, last_nonpad_indices, :] # here we select all batches, only the final non-padding response token for each batch, and the full hidden state for this final token\n",
        "    values = self.critic(last_hidden.detach().float()).squeeze(-1) # here we generate the reward prediction based on only the final hidden state per sequence\n",
        "\n",
        "    # keep rewards scalar per sequence\n",
        "    # rewards is of shape (batch_size, 1), so here we convert it to a 1D vector with length batch_size\n",
        "    rewards = rewards.view(-1).float().to(self.device)\n",
        "    values = values.view(-1)\n",
        "\n",
        "    # ---- Advantage computation ----\n",
        "    advantages = rewards - values.detach()\n",
        "    # broadcast advantage to match token dimension for policy loss\n",
        "    # advantages is of shape (batch_size, seq_len - 1)\n",
        "    advantages = advantages.unsqueeze(1).expand_as(logprobs_actor)\n",
        "\n",
        "    # ---- PPO policy loss ----\n",
        "    # here we are directly mirroring the policy loss function for PPO\n",
        "    # here we are comparing logprobs n to mask n + 1, because the nth logprob refers to the n+1'th token\n",
        "    policy_loss = -1 * self.masked_mean(logprobs_actor * advantages, mask[:, 1:])\n",
        "\n",
        "    # ---- Value loss ----\n",
        "    # here we are directly mirroring the value loss function for PPO\n",
        "    # mean squared error\n",
        "    value_loss = F.mse_loss(values, rewards)\n",
        "\n",
        "    # ---- Entropy ----\n",
        "    # here we are directly mirroring the entropy equation for PPO\n",
        "    # we don't want to use logprob_actor, because this only represents the logprobs for the CHOSEN token. we want the logprob distributions for all tokens at each step in the sequence\n",
        "    logprobs_full = F.log_softmax(logits[:, :-1], dim = -1) # shape (batch_size, seq_len - 1, vocab_size)\n",
        "    probs = logprobs_full.exp() # shape (batch_size, seq_len - 1, vocab_size)\n",
        "    token_entropy = -1 * (probs * logprobs_full).sum(dim = -1) # shape (batch_size, seq_len - 1)\n",
        "    entropy = self.masked_mean(token_entropy, mask[:, 1:]) # scalar\n",
        "\n",
        "    # ---- KL ----\n",
        "    # here we are directly mirroring the KL equation for PPO\n",
        "    kl = self.masked_mean(logprobs_actor - logprobs_ref, mask[:, 1:])\n",
        "\n",
        "    # ---- Actor Optimization ----\n",
        "    self.actor_optimizer.zero_grad() # resets all stored gradients for the model; each PPO update step should reflect only the current batch's loss\n",
        "    actor_loss = policy_loss - self.entropy_coef * entropy + self.kl_coef * kl # build the total loss function for actor; minimize policy loss, maximize entropy, minimize kl drift\n",
        "    actor_loss.backward() # computes the gradients of actor_loss, with respect to all the parameters in the actor model\n",
        "    torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm) # clips the gradients so that their total does not exceed max_grad_norm\n",
        "    self.actor_optimizer.step() # uses AdamW, a type of gradient descent, to update the parameters of the actor model\n",
        "\n",
        "    # ---- Critic Optimization ----\n",
        "    self.critic_optimizer.zero_grad() # resets all stored gradients for the model; each PPO update step should reflect only the current batch's loss\n",
        "    critic_loss = value_loss * self.value_coef # build the total loss function for critic; minimize value loss\n",
        "    critic_loss.backward() # computes the gradients of critic_loss, with respect to all the parameters in the critic head\n",
        "    torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm) # clips the gradients so that their total does not exceed max_grad_norm\n",
        "    self.critic_optimizer.step() # uses AdamW, a type of gradient descent, to update the parameters of the critic head\n",
        "\n",
        "    # ---- Adaptive KL ----\n",
        "    # kl: current average kl divergence between the actor and the reference\n",
        "    # target_kl: the desired kl level, i.e. how much divergence we are okay with\n",
        "    # kl_coef: the penalty strength applied during actor loss; how much we want to prevent drift during optimization\n",
        "    # we want the actor to improve rewards, but not diverge too far from the reference model\n",
        "    # high kl_coef => stronger penalty, so actor stays close to reference. low kl_coef => weaker penalty, so actor is allowed to explore more\n",
        "    # however, the ideal kl_coef changes dynamically, because different optimization steps result in different levels of updates\n",
        "    # if the current kl divergence is too large (1.5x our target), we increase the penalty\n",
        "    # if the current kl divergence is too small (1/1.5 our target), we decrease the penalty\n",
        "    with torch.no_grad():\n",
        "      if kl.item() > 1.5 * self.target_kl:\n",
        "        self.kl_coef *= 1.5\n",
        "      elif kl.item() < (self.target_kl / 1.5):\n",
        "        self.kl_coef /= 1.5\n",
        "\n",
        "    # ---- Logging ----\n",
        "    stats = {\n",
        "        \"policy_loss\": policy_loss.item(),\n",
        "        \"value_loss\": value_loss.item(),\n",
        "        \"entropy\": entropy.item(),\n",
        "        \"kl\": kl.item(),\n",
        "        \"kl_coef\": self.kl_coef,\n",
        "        \"actor_loss\": actor_loss.item(),\n",
        "        \"critic_loss\": critic_loss.item(),\n",
        "        \"average_reward\": average_reward\n",
        "    }\n",
        "    wandb.log(stats)\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkDZ6ck2O26N",
        "outputId": "916ccb44-3a4d-45aa-8df5-cab79d896a8d"
      },
      "outputs": [],
      "source": [
        "class PGSRM:\n",
        "\n",
        "  def __init__(self, parent_model, metric, task, max_retries=3, retry_delay=2):\n",
        "    self.client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    self.parent_model = parent_model\n",
        "    self.metric = metric  # \"cosine\" or \"euclidean\"\n",
        "    self.task = task\n",
        "    self.max_retries = max_retries\n",
        "    self.retry_delay = retry_delay\n",
        "    self.log_file = \"episode_logs_gpt2_pgsrm.jsonl\"\n",
        "\n",
        "    self.parent_cache = {}\n",
        "    self.embedding_cache = {}   # NEW: cache embeddings\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  def safe_api_call(self, func, *args, **kwargs):\n",
        "    for attempt in range(1, self.max_retries + 1):\n",
        "      try:\n",
        "        return func(*args, **kwargs)\n",
        "      except Exception as e:\n",
        "        print(f\"[Warning] API call failed (attempt {attempt}/{self.max_retries}): {e}\")\n",
        "        if attempt < self.max_retries:\n",
        "          wait_time = self.retry_delay * (2 ** (attempt - 1))\n",
        "          print(f\"Retrying in {wait_time:.1f} seconds...\")\n",
        "          time.sleep(wait_time)\n",
        "        else:\n",
        "          print(\"[Error] Max retries reached. Returning None.\")\n",
        "          return None\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # PARENT GENERATION (unchanged)\n",
        "  def parent_generate(self, input_text):\n",
        "    if input_text in self.parent_cache:\n",
        "      return self.parent_cache[input_text]\n",
        "\n",
        "    system_prompt = f\"\"\"\n",
        "    {self.task}\n",
        "    You will receive the input data as the value for the INPUT key.\n",
        "    You must provide your response in a JSON format, with OUTPUT as the only key, and your response as the value.\n",
        "    Your response must be in this format: {{\"OUTPUT\": \"<your response>\"}}.\n",
        "    \"\"\"\n",
        "\n",
        "    input_prompt = f'INPUT: \"{input_text}\"\\nYOUR RESPONSE =>'\n",
        "\n",
        "    response = self.safe_api_call(\n",
        "      self.client.chat.completions.create,\n",
        "      model=self.parent_model,\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": input_prompt},\n",
        "      ],\n",
        "      temperature=0.0,\n",
        "    )\n",
        "\n",
        "    if response is None:\n",
        "      return \"\"\n",
        "\n",
        "    raw_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    try:\n",
        "      parsed = json.loads(raw_text)\n",
        "      parent_text = parsed.get(\"OUTPUT\", \"\").strip()\n",
        "    except:\n",
        "      parent_text = raw_text\n",
        "\n",
        "    self.parent_cache[input_text] = parent_text\n",
        "    return parent_text\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # NEW: EMBEDDING VIA OPENAI text-embedding-3-large\n",
        "  def embed(self, text, is_euclidean):\n",
        "\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    # cached?\n",
        "    if text in self.embedding_cache:\n",
        "      return self.embedding_cache[text]\n",
        "\n",
        "    # call embedding model\n",
        "    response = self.safe_api_call(\n",
        "      self.client.embeddings.create,\n",
        "      model=\"text-embedding-3-large\",\n",
        "      input=text\n",
        "    )\n",
        "\n",
        "    if response is None:\n",
        "      print(\"[Error] Embedding API failed; returning zeros\")\n",
        "      vec = torch.zeros(3072)\n",
        "    else:\n",
        "      vec_list = response.data[0].embedding\n",
        "      vec = torch.tensor(vec_list, dtype=torch.float32)\n",
        "\n",
        "    if is_euclidean:\n",
        "      vec = F.normalize(vec, dim=0)\n",
        "\n",
        "    self.embedding_cache[text] = vec\n",
        "    return vec\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  def get_reward(self, input_text, child_output, episode, is_test=False):\n",
        "\n",
        "    parent_output = self.parent_generate(input_text)\n",
        "    is_euclidean = (self.metric == \"euclidean\")\n",
        "\n",
        "    parent_vec = self.embed(parent_output, is_euclidean)\n",
        "    child_vec = self.embed(child_output, is_euclidean)\n",
        "\n",
        "    if self.metric == \"cosine\":\n",
        "      sim = F.cosine_similarity(parent_vec, child_vec, dim=0).item()\n",
        "      sim = (sim + 1) / 2\n",
        "      reward = (sim ** 4)\n",
        "      reward = float(torch.clamp(torch.tensor(reward), 0.0, 1.0))\n",
        "\n",
        "    elif self.metric == \"euclidean\":\n",
        "      dist = torch.norm(parent_vec - child_vec, p=2).item()\n",
        "      reward = 1 - dist\n",
        "      reward = float(torch.clamp(torch.tensor(reward), -1.0, 1.0))\n",
        "\n",
        "    # logging\n",
        "    log = (\n",
        "      f\"EPISODE {episode} | \"\n",
        "      f\"Input: {input_text} | \"\n",
        "      f\"Parent: {parent_output} | \"\n",
        "      f\"Child: {child_output} | \"\n",
        "      f\"Reward: {reward:.3f}\\n\"\n",
        "    )\n",
        "\n",
        "    if not is_test:\n",
        "      with open(self.log_file, \"a\") as f:\n",
        "        f.write(json.dumps(log) + \"\\n\\n\")\n",
        "    else:\n",
        "      print(log)\n",
        "\n",
        "    return reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "task = \"\"\"\n",
        "You are an AI assistant that copies a sentence exactly as given.\n",
        "\n",
        "You will receive an INPUT containing a short English sentence.\n",
        "Your job is to output the exact same sentence â€” unchanged.\n",
        "\n",
        "Rules:\n",
        "- Do not change, paraphrase, simplify, or summarize the sentence.\n",
        "- Copy it exactly as-is.\n",
        "- Keep punctuation, capitalization, and spacing identical.\n",
        "- Output must appear as {\"OUTPUT\": \"<sentence>\"} exactly.\n",
        "\"\"\"\n",
        "\n",
        "reward_model = PGSRM(\n",
        "    parent_model=\"gpt-4o-mini\",\n",
        "    metric=\"cosine\",\n",
        "    task=task\n",
        ")\n",
        "\n",
        "input_text = \"Copy this sentence: 'The sun slowly rises over the calm blue ocean.'\"\n",
        "\n",
        "options = [\n",
        "    \"The sun slowly rises over the calm blue ocean.\",  # perfect\n",
        "    \"The sun rises over the calm blue ocean.\",         # slight change\n",
        "    \"The sun slowly fuckkkfm jesus obamamamama.\",            # incorrect\n",
        "    \"jacket pkenty brown mormon\",                              # terrible\n",
        "]\n",
        "\n",
        "for option in options:\n",
        "    reward_model.get_reward(input_text, option, episode=0, is_test=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkI75lomvK0M",
        "outputId": "06b84eee-6ad2-4fb6-d2f9-c10b9b1cd2ea"
      },
      "outputs": [],
      "source": [
        "sentence_list = pd.read_csv(\"sentences.csv\")\n",
        "sentence_list = sentence_list['sentences'].tolist()\n",
        "print(sentence_list[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "lMvAReVKvbUp",
        "outputId": "7051f3f0-d66a-49cb-a3a2-fd972001ebfb"
      },
      "outputs": [],
      "source": [
        "## Training Loop\n",
        "\n",
        "base_model = \"gpt2-large\"\n",
        "BATCH_SIZE = 50\n",
        "EPISODES = 100000\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dtype = torch.float32\n",
        "\n",
        "actor_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=dtype,               # defines precision used for weights\n",
        "    device_map={\"\": \"cuda:0\"},       # loads the model entirely onto GPU 0\n",
        "    low_cpu_mem_usage=True,          # reduces RAM load during model init\n",
        ")\n",
        "actor_model_device = next(actor_model.parameters()).device\n",
        "\n",
        "ref_base = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=dtype,\n",
        "    device_map={\"\": \"cuda:0\"},\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    actor_model = actor_model,\n",
        "    ref_model = ref_base,\n",
        "    tokenizer = tokenizer,\n",
        "    device = actor_model_device,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    entropy_coef = 0.01,\n",
        "    clip_range = 0.4,\n",
        "    kl_coef = 0.00005,\n",
        "    target_kl = 0.8,\n",
        "    value_coef = 0.5, # standard value\n",
        "    max_grad_norm = 1, # standard for keeping stable updates,\n",
        "    critic_learning_rate = 1e-4 # high learning rate for the critic, such that it picks up on variance in rewards quickly\n",
        ")\n",
        "\n",
        "prompt_tensors, response_tensors, rewards = [], [], []\n",
        "\n",
        "for episode in range(1, EPISODES + 1):\n",
        "\n",
        "  if episode % 100 == 0:\n",
        "    print(f\"[alive] Episode {episode}\", flush=True)\n",
        "\n",
        "  sentence = random.choice(sentence_list)\n",
        "\n",
        "  full_input_state = f\"Copy this sentence: '{sentence}'\"\n",
        "  q = tokenizer(full_input_state, return_tensors=\"pt\", padding=True).to(actor_model_device)\n",
        "  query_tensors = q[\"input_ids\"]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    gen = ppo_trainer.actor.generate(\n",
        "        query_tensors,\n",
        "        max_new_tokens = 10,\n",
        "        do_sample = True,\n",
        "        temperature = 1.0,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  response_ids = gen[:, query_tensors.size(1):]\n",
        "  gen_txt = tokenizer.batch_decode(response_ids, skip_special_tokens = True)[0]\n",
        "\n",
        "  # PGSRL reward calculation\n",
        "  reward = reward_model.get_reward(full_input_state, gen_txt, episode)\n",
        "  reward_t = torch.tensor([float(reward)], dtype=torch.float, device=actor_model_device)\n",
        "\n",
        "  prompt_tensors.append(query_tensors)\n",
        "  response_tensors.append(response_ids)\n",
        "  rewards.append(reward_t)\n",
        "\n",
        "  if episode % BATCH_SIZE == 0:\n",
        "\n",
        "    # Pad variable-length tensors to same length\n",
        "    prompts_batch = pad_sequence(\n",
        "        [p.squeeze(0) for p in prompt_tensors], batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "    )\n",
        "    responses_batch = pad_sequence(\n",
        "        [r.squeeze(0) for r in response_tensors], batch_first=True, padding_value=tokenizer.pad_token_id\n",
        "    )\n",
        "    rewards_batch = torch.cat(rewards, dim=0)\n",
        "\n",
        "    average_reward = rewards_batch.mean().item()\n",
        "\n",
        "    stats = ppo_trainer.step(prompts_batch, responses_batch, rewards_batch, average_reward)\n",
        "\n",
        "    prompt_tensors, response_tensors, rewards = [], [], []\n",
        "\n",
        "message = f\"copy-gpt2-pgsrm: {EPISODES} episodes completed\"\n",
        "url = f\"https://api.telegram.org/bot{bot_id}/sendMessage\"\n",
        "payload = {\n",
        "    \"chat_id\": chat_id,\n",
        "    \"text\": message\n",
        "}\n",
        "requests.post(url, json = payload)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
